{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations with IBM data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will be using real data from the IBM Watson Studio platform to build recommendation systems. \n",
    "\n",
    "Let's get started by importing the necessary libraries and reading in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/nabanita/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nabanita/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/nabanita/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nabanita/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nltk.download(['punkt', 'stopwords', 'wordnet', 'averaged_perceptron_tagger'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/user-item-interactions.csv')\n",
    "df_content = pd.read_csv('data/articles_community.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at each of the dataframes closely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1430.0</td>\n",
       "      <td>using pixiedust for fast, flexible, and easier...</td>\n",
       "      <td>ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1314.0</td>\n",
       "      <td>healthcare python streaming application demo</td>\n",
       "      <td>083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1429.0</td>\n",
       "      <td>use deep learning for image classification</td>\n",
       "      <td>b96a4f2e92d8572034b1e9b28f9ac673765cd074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1338.0</td>\n",
       "      <td>ml optimization using cognitive assistant</td>\n",
       "      <td>06485706b34a5c9bf2a0ecdac41daf7e7654ceb7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1276.0</td>\n",
       "      <td>deploy your python model as a restful api</td>\n",
       "      <td>f01220c46fc92c6e6b161b1849de11faacd7ccb2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  article_id                                              title  \\\n",
       "0           0      1430.0  using pixiedust for fast, flexible, and easier...   \n",
       "1           1      1314.0       healthcare python streaming application demo   \n",
       "2           2      1429.0         use deep learning for image classification   \n",
       "3           3      1338.0          ml optimization using cognitive assistant   \n",
       "4           4      1276.0          deploy your python model as a restful api   \n",
       "\n",
       "                                      email  \n",
       "0  ef5f11f77ba020cd36e1105a00ab868bbdbf7fe7  \n",
       "1  083cbdfa93c8444beaa4c5f5e0f5f9198e4f9e0b  \n",
       "2  b96a4f2e92d8572034b1e9b28f9ac673765cd074  \n",
       "3  06485706b34a5c9bf2a0ecdac41daf7e7654ceb7  \n",
       "4  f01220c46fc92c6e6b161b1849de11faacd7ccb2  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>doc_status</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>Detect bad readings in real time using Python ...</td>\n",
       "      <td>Detect Malfunctioning IoT Sensors with Streami...</td>\n",
       "      <td>Live</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...</td>\n",
       "      <td>See the forest, see the trees. Here lies the c...</td>\n",
       "      <td>Communicating data science: A guide to present...</td>\n",
       "      <td>Live</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...</td>\n",
       "      <td>Here’s this week’s news in Data Science and Bi...</td>\n",
       "      <td>This Week in Data Science (April 18, 2017)</td>\n",
       "      <td>Live</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...</td>\n",
       "      <td>Learn how distributed DBs solve the problem of...</td>\n",
       "      <td>DataLayer Conference: Boost the performance of...</td>\n",
       "      <td>Live</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>Skip navigation Sign in SearchLoading...\\r\\n\\r...</td>\n",
       "      <td>This video demonstrates the power of IBM DataS...</td>\n",
       "      <td>Analyze NY Restaurant data using Spark in DSX</td>\n",
       "      <td>Live</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           doc_body  \\\n",
       "0           3  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "1           5  No Free Hunch Navigation * kaggle.com\\r\\n\\r\\n ...   \n",
       "2           7  ☰ * Login\\r\\n * Sign Up\\r\\n\\r\\n * Learning Pat...   \n",
       "3           8  DATALAYER: HIGH THROUGHPUT, LOW LATENCY AT SCA...   \n",
       "4          12  Skip navigation Sign in SearchLoading...\\r\\n\\r...   \n",
       "\n",
       "                                     doc_description  \\\n",
       "0  Detect bad readings in real time using Python ...   \n",
       "1  See the forest, see the trees. Here lies the c...   \n",
       "2  Here’s this week’s news in Data Science and Bi...   \n",
       "3  Learn how distributed DBs solve the problem of...   \n",
       "4  This video demonstrates the power of IBM DataS...   \n",
       "\n",
       "                                       doc_full_name doc_status  article_id  \n",
       "0  Detect Malfunctioning IoT Sensors with Streami...       Live           0  \n",
       "1  Communicating data science: A guide to present...       Live           1  \n",
       "2         This Week in Data Science (April 18, 2017)       Live           2  \n",
       "3  DataLayer Conference: Boost the performance of...       Live           3  \n",
       "4      Analyze NY Restaurant data using Spark in DSX       Live           4  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_content['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "In this section, we perform EDA on both of our dataframes. We start with the dataframe 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45993, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out what are the dtypes of each column and whether there is any null value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 45993 entries, 0 to 45992\n",
      "Data columns (total 3 columns):\n",
      "article_id    45993 non-null float64\n",
      "title         45993 non-null object\n",
      "email         45976 non-null object\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only the 'email' column has (45993-45976)=17 null values. Next we ask how many unique values are there in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id     714\n",
       "title          714\n",
       "email         5148\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, unique values in each column is much smaller than the total number of samples. This is, however, is expected as:\n",
    "\n",
    "- An article might have multiple interactions with different as well as same users.\n",
    "- A user might have interacted with different articles and/or same articles multiple times.\n",
    "\n",
    "Note that, the column 'email' contains the user information but it is quite messy to handle. Each unique user will have a unique email. Therefore, we can use the information about email to translate it into unique user ids. \n",
    "\n",
    "There were a small number of null values, and it was found that all of these null values likely belonged to a single user (which is how they are stored using the function below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def email_to_id():\n",
    "    \n",
    "    \"\"\"\n",
    "    A function that maps unique user emails to unique user ids.\n",
    "    \n",
    "    Parameter\n",
    "    ------------\n",
    "    None\n",
    "    \n",
    "    Returns\n",
    "    ------------\n",
    "    coded_dict : dict\n",
    "        A dictionary whose keys are user emails and values are corresponding user ids.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    user_id = 1\n",
    "    coded_dict = {}\n",
    "\n",
    "    for email in df['email'].unique():\n",
    "        coded_dict[email] = user_id\n",
    "        user_id += 1\n",
    "       \n",
    "    return coded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "coded_dict = email_to_id()\n",
    "df['user_id'] = df['email'].apply(lambda x:coded_dict[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new column 'user_id' is introduced that holds user ids. Now, we can get rid of the 'email' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('email', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "article_id    0\n",
       "title         0\n",
       "user_id       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So after this conversion, there is no null value in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now move to the other dataframe 'df_content' and extract out some information from it as we did for the dataframe 'df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1056, 5)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1056 entries, 0 to 1055\n",
      "Data columns (total 5 columns):\n",
      "doc_body           1042 non-null object\n",
      "doc_description    1053 non-null object\n",
      "doc_full_name      1056 non-null object\n",
      "doc_status         1056 non-null object\n",
      "article_id         1056 non-null int64\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 41.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df_content.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "doc_body           1036\n",
       "doc_description    1022\n",
       "doc_full_name      1051\n",
       "doc_status            1\n",
       "article_id         1051\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note the following:\n",
    "\n",
    "- Only two columns 'doc_body' and 'doc_description' have null values.\n",
    "- Although 'article_id' and 'doc_full_name' have no null value, number of unique entries in each of these columns is less than the total entries. Since the dataframe only holds information about articles and their content, this might be an indication of the presence of duplicate rows.\n",
    "- The column 'doc_status' has only one unique value and hence can be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content.drop('doc_status', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check for duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Follow Sign in / Sign up Home About Insight Da...</td>\n",
       "      <td>During the seven-week Insight Data Engineering...</td>\n",
       "      <td>Graph-based machine learning</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>692</th>\n",
       "      <td>Homepage Follow Sign in / Sign up Homepage * H...</td>\n",
       "      <td>One of the earliest documented catalogs was co...</td>\n",
       "      <td>How smart catalogs can turn the big data flood...</td>\n",
       "      <td>221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>Homepage Follow Sign in Get started Homepage *...</td>\n",
       "      <td>Today’s world of data science leverages data f...</td>\n",
       "      <td>Using Apache Spark as a parallel processing fr...</td>\n",
       "      <td>398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>This video shows you how to construct queries ...</td>\n",
       "      <td>Use the Primary Index</td>\n",
       "      <td>577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>Homepage Follow Sign in Get started * Home\\r\\n...</td>\n",
       "      <td>If you are like most data scientists, you are ...</td>\n",
       "      <td>Self-service data preparation with IBM Data Re...</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              doc_body  \\\n",
       "365  Follow Sign in / Sign up Home About Insight Da...   \n",
       "692  Homepage Follow Sign in / Sign up Homepage * H...   \n",
       "761  Homepage Follow Sign in Get started Homepage *...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "971  Homepage Follow Sign in Get started * Home\\r\\n...   \n",
       "\n",
       "                                       doc_description  \\\n",
       "365  During the seven-week Insight Data Engineering...   \n",
       "692  One of the earliest documented catalogs was co...   \n",
       "761  Today’s world of data science leverages data f...   \n",
       "970  This video shows you how to construct queries ...   \n",
       "971  If you are like most data scientists, you are ...   \n",
       "\n",
       "                                         doc_full_name  article_id  \n",
       "365                       Graph-based machine learning          50  \n",
       "692  How smart catalogs can turn the big data flood...         221  \n",
       "761  Using Apache Spark as a parallel processing fr...         398  \n",
       "970                              Use the Primary Index         577  \n",
       "971  Self-service data preparation with IBM Data Re...         232  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content[df_content.duplicated('article_id')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are 5 duplicate rows and we drop them keeping only their first occurences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_content.drop_duplicates('article_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a quick check that everything is as per expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_body</th>\n",
       "      <th>doc_description</th>\n",
       "      <th>doc_full_name</th>\n",
       "      <th>article_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [doc_body, doc_description, doc_full_name, article_id]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_content[df_content.duplicated('article_id')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Rank-Based Recommendations\n",
    "\n",
    "In this type of recommendation technique, recommendations are provided based on the popularity of the associated items, *e.g.*, movies with higher ratings, songs with maximum number of downloads and so on.\n",
    "\n",
    "Note that, we do not actually have ratings for whether a user liked an article or not in our dataset. We only know that a user has interacted with an article. In this situation, the popularity of an article can really only be based on how often an article was interacted with.\n",
    "\n",
    "The following function recommends articles based on their popularities amoung users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(n, df=df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function determines the most popular articles based on the number of interactions and returns\n",
    "    the corresponding ids and names.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    n : int\n",
    "     The number of top articles to return\n",
    "     \n",
    "    df : pandas dataframe\n",
    "      dataframe as defined at the top of the notebook from the file user-item-interactions.csv \n",
    "    \n",
    "    Returns\n",
    "    -----------\n",
    "    top_articles_id : list\n",
    "       A list of the top 'n' article ids\n",
    "       \n",
    "    top_articles : list \n",
    "       A list of the top 'n' article names    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    top_articles_id = list(df.groupby('article_id').count()['user_id'].sort_values(ascending=False).index[:n])\n",
    "    top_articles = list(df[df['article_id'].isin(top_articles_id)]['title'].unique())\n",
    "    \n",
    "    return top_articles_id, top_articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some recommendations for any arbitrary user (note that this method is independent of user profile)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 5 articles using rank-based recommendation technique are :\n",
      "\n",
      "['use deep learning for image classification', 'predicting churn with the spss random tree algorithm', 'visualize car data with brunel', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'insights from new york car accident reports']\n"
     ]
    }
   ],
   "source": [
    "print(\"top 5 articles using rank-based recommendation technique are :\")\n",
    "print()\n",
    "print(get_top_articles(5)[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part III: User-User Based Collaborative Filtering\n",
    "\n",
    "In this type of recommendation technique, we look for users who are closely related to each other in terms of their interactions with items, *e.g.*, users who have watched same movies and have given ratings in the same ballpark are `neighbors` in the movie ratings plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by constructing a new column in the dataframe 'df' which will return the value 1 for every user-article interaction pair. If a user has interacted with the same article multiple times, for each interaction there will be one value (which is 1) in this column. We call this new column 'intercation'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['interaction'] = [1 for title in df['title']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build our recommendation system based on user-user collaborative filtering step-by-step as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "Let's define the following function to create user-item matrix whose rows are unique user ids, columns are unique article ids and entries are number of interactions between users and articles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns a matrix which holds the information about interactions between users and articles.\n",
    "    \n",
    "    Parameter\n",
    "    ----------\n",
    "    df : pandas dataframe \n",
    "       dataframe as defined at the top of the notebook from the file user-item-interactions.csv\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    user_item : matrix\n",
    "       a matrix whose rows are unique user ids, columns are unique article ids and entries in each cell is the\n",
    "       number of interactions between the corresponding user and article. We fill all the null values with 0 to \n",
    "       denote no interaction between the corresponding users and articles. \n",
    "  \n",
    "    \"\"\"\n",
    "    \n",
    "    user_item = df.groupby(['user_id', 'article_id'])['interaction'].min().unstack().fillna(0)\n",
    "    \n",
    "    return user_item "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item = create_user_item_matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 \n",
    "\n",
    "Next we construct a function which, given a user id, provides a dataframe containing all users similar to the input user sorted according to their similarity scores first and then by the total number of interactions. The returned dataframe does not contain the input user id as we know that each user is most similar to itself. We compute the similarity between two users by taking dot product between their interaction vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_users(user_id, df=df, user_item=user_item):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function returns a dataframe which contains all the neighbors of the input user sorted according to their\n",
    "    similarity scores first and then by the total number of interactions.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    user_id : int\n",
    "         an input user id\n",
    "        \n",
    "    df : pandas dataframe\n",
    "         dataframe as defined at the top of the notebook from the file user-item-interactions.csv\n",
    "    \n",
    "    user_item : (pandas dataframe) matrix \n",
    "         a users by articles matrix where non-zero entries represents that a user has interacted with an article\n",
    "         and 0 stands for no interaction.\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    neighbors_df : pandas dataframe\n",
    "         a dataframe with the following columns:\n",
    "         1. neighbor_id : a neighbor user_id\n",
    "         2. similarity_score : measure of the similarity between the input user and its neighbors\n",
    "         3. total_interactions : the number of articles viewed by a neighbor user\n",
    "   \n",
    "    \"\"\"\n",
    "    most_similar_users, similarity_score = [], []\n",
    "    \n",
    "    # loop through all other users    \n",
    "    for other_id in range(len(user_item)):\n",
    "        if other_id != user_id-1: # since the indexing start from zero in python\n",
    "            # store the similarity score for every other user\n",
    "            similarity_score.append(np.dot(user_item.iloc[user_id-1, :], user_item.iloc[other_id, :]))\n",
    "            # store the id of every other user\n",
    "            most_similar_users.append(other_id+1)\n",
    "\n",
    "    # store the total number interactions for each similar user\n",
    "    total_interactions = [df.groupby('user_id').count()['interaction'].values[id-1] for id in most_similar_users]\n",
    "    \n",
    "    # construct the dataframe\n",
    "    neighbors_df = pd.DataFrame([most_similar_users, similarity_score, total_interactions]).transpose() \n",
    "    neighbors_df.columns = ['neighbor_id', 'similarity_score', 'total_interactions']\n",
    "    # sort first by similarity score and then by number of interactions\n",
    "    neighbors_df.sort_values(by=['similarity_score','total_interactions'], ascending=False, inplace=True)\n",
    "    \n",
    "    return neighbors_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "In this step, we construct the following three functions that will be helpful when making recommendations later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_names(article_ids, df=df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function, given a list of article ids, returns a corresponding list containing the article titles.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    article_ids : list\n",
    "        a list of article ids\n",
    "        \n",
    "    df : pandas dataframe\n",
    "        dataframe as defined at the top of the notebook from the file user-item-interactions.csv\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    article_names : list\n",
    "          a list of article names associated with the input list of article ids \n",
    "                    \n",
    "    \"\"\"\n",
    "    \n",
    "    article_names = df[df['article_id'].isin(article_ids)]['title'].unique()\n",
    "    \n",
    "    return list(article_names) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_articles(user_id, df=df):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function provides a list of article ids and corresponding titles that a user has interacted with.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    user_id : int\n",
    "        an input user id\n",
    "        \n",
    "    df : pandas dataframe\n",
    "        dataframe as defined at the top of the notebook from the file user-item-interactions.csv\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    article_ids : list\n",
    "          a list of the article ids that the user has interacted with\n",
    "          \n",
    "    article_names : list\n",
    "          a list of article names associated with the list article_ids \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    article_ids = df.query('user_id==@user_id')['article_id'].unique()\n",
    "    article_names = get_article_names(article_ids)\n",
    "    \n",
    "    return article_ids, article_names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_sorted_articles(article_ids, df=df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given a list of article ids, this function sorts them according to the number of interactions they have with\n",
    "    users in a descending order.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    article_ids : list\n",
    "        a list of article ids\n",
    "        \n",
    "    df : pandas dataframe\n",
    "        dataframe as defined at the top of the notebook from the file user-item-interactions.csv\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    sorted_article_ids : array\n",
    "         an array of article ids sorted according to the number of interactions\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df_new = df.groupby('article_id').count().reset_index()[['article_id', 'interaction']].sort_values('interaction', ascending=False)\n",
    "    sorted_article_ids = df_new[df_new['article_id'].isin(article_ids)]['article_id'].values\n",
    "    \n",
    "    return sorted_article_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "In the final step, we write a function that will make recommendation for a given user based on the collaborative filtering using the following steps :\n",
    "\n",
    "- find out the articles that our input user has already read using the function 'get_user_articles'\n",
    "- list all the users similar to the input user determined from the dataframe returned by the function 'get_top_sorted_users'\n",
    "- For every similar user \n",
    "    - find out the articles that he/she has read but our input user has not\n",
    "    - sort these articles using the function 'get_top_sorted_articles' in a descending order of popularity\n",
    "    - include them in the recommendation list\n",
    "    \n",
    "The above described method taked into consideration the following two things :\n",
    "\n",
    "* Instead of arbitrarily choosing when there are neighbor users with the same similarity score, we choose the users with more number of total article interactions before choosing those with fewer article interactions.\n",
    "\n",
    "* Instead of arbitrarily choosing articles from a neighbor user, we choose articles with more number of total interactions before choosing those with fewer total interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_user_recs(user_id, n=10):\n",
    "\n",
    "    \"\"\"\n",
    "    A function to make recommendations based on user-user collaborative filtering. Loops through all the users \n",
    "    based on the similarity to the input user. For each user, we find articles the input user has not interacted \n",
    "    with before but its neighbor has and provide them as recommendations. Note that if users have the same \n",
    "    similarity score, users with more total interactions are chosen before those with fewer total interactions. \n",
    "    Also we choose articles with more number of interactions before those with fewer interactions.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    user_id : int\n",
    "         an input user id for whom recommendations are to be made\n",
    "         \n",
    "    n : int\n",
    "        the number of recommendations we want to make for the input user\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    recs : numpy array\n",
    "         an array of recommended article ids for the input user\n",
    "         \n",
    "    rec_names : list\n",
    "         a list of names of the recommended articles\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # articles that our input user has interacted with    \n",
    "    articles_read = get_user_articles(user_id)[0]\n",
    "    \n",
    "    # users similar to our input user sorted according to the similarity score and interaction numbers\n",
    "    neighbors_df = get_top_sorted_users(user_id)\n",
    "    similar_users = neighbors_df['neighbor_id'].values\n",
    "    \n",
    "    # list to store all the recommendations\n",
    "    recs = []\n",
    "    \n",
    "    for user in similar_users:\n",
    "        # articles that have no interaction with the input user but have interactions with similar users\n",
    "        articles_not_read = np.setdiff1d(get_user_articles(user)[0], articles_read)\n",
    "        \n",
    "        # sort these articles according to the number of interactions in descending order\n",
    "        articles_not_read_sorted = get_top_sorted_articles(articles_not_read)\n",
    "        \n",
    "        # now add these articles to the list 'recs'\n",
    "        recs.extend(articles_not_read_sorted)\n",
    "        \n",
    "        # if the length of the array exceeds the max. number of recommendations we want to make, break the loop\n",
    "        if len(set(recs)) > n:\n",
    "            break\n",
    "    \n",
    "    # consider only the unique values in the recommendation list\n",
    "    recs = list(set(recs))\n",
    "    rec_names = get_article_names(recs)       \n",
    "            \n",
    "    # return the first 'm' elements of our recommendation array in case it has more than m entries\n",
    "    return recs[:n], rec_names[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use our recommendation system to make recommendation for some arbitrarily chosen user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 10 recommendations for user 20 are the following article ids:\n",
      "[1024.0, 1409.0, 1410.0, 1411.0, 1152.0, 1157.0, 1154.0, 1153.0, 1160.0, 1162.0]\n",
      "\n",
      "The top 10 recommendations for user 20 are the following article names:\n",
      "['ml optimization using cognitive assistant', 'deploy your python model as a restful api', 'apache spark lab, part 1: basic concepts', 'timeseries data analysis of iot events by using jupyter notebook', 'dsx: hybrid mode', 'predicting churn with the spss random tree algorithm', 'analyze energy consumption in buildings', 'ibm watson facebook posts for 2015', 'use xgboost, scikit-learn & ibm watson machine learning apis', 'apache spark lab, part 3: machine learning']\n"
     ]
    }
   ],
   "source": [
    "rec_ids, rec_names = user_user_recs(20, 10)\n",
    "print(\"The top 10 recommendations for user 20 are the following article ids:\")\n",
    "print(rec_ids)\n",
    "print()\n",
    "print(\"The top 10 recommendations for user 20 are the following article names:\")\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV: Content Based Recommendations\n",
    "\n",
    "In content based recommendations, we use features or information about items, users, *e.g.*, recommending movies on the basis of its genres is an example of content based recommendation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the columns 'article_id' and 'doc_full_name' from the original dataframe df_content. We will use the column 'doc_full_name' to extract out content related information from articles.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_content = df_content[['doc_full_name', 'article_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now build our recommendation system based on content step-by-step as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1\n",
    "\n",
    "First, we construct a function that will convert a text to a list of tokens and will use this function to extarct out content or important features associated with an article. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_word(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function to clean an input text. The steps followed for the text cleaning are :\n",
    "     \n",
    "     1. Normalization i.e. conversion to lower case and punctuation removal\n",
    "     2. Tokenization \n",
    "     3. Stop words removal\n",
    "     4. Lemmatization\n",
    "    \n",
    "    Parameter \n",
    "    -----------\n",
    "      text : str \n",
    "        the input text to be cleaned\n",
    "      \n",
    "    Returns \n",
    "    ----------\n",
    "      lemm_token_list : list\n",
    "             a list of tokens obtained after cleaning the text\n",
    "        \n",
    "    \"\"\"    \n",
    "    \n",
    "    token_list = word_tokenize(re.sub(r\"[^a-z0-9]\", \" \", text.lower()))\n",
    "    token_nostop_list = [token for token in token_list if token not in stopwords.words(\"english\")]\n",
    "    pos_dict = {\"N\":wordnet.NOUN, \"J\":wordnet.ADJ, \"V\":wordnet.VERB, \"R\":wordnet.ADV}\n",
    "    \n",
    "    lemm_token_list = set()\n",
    "    for token,pos in nltk.pos_tag(token_nostop_list):\n",
    "        try:\n",
    "            lemm_token_list.add(WordNetLemmatizer().lemmatize(token, pos_dict[pos[0]]))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return list(lemm_token_list)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "We will now use sklearn's CountVectorizer on the 'doc_full_name' column to construct the document-term matrix where the rows are unique article names present in the dataset and the columns are tokens of the vocabulary constructed from the same dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "countvec = CountVectorizer(analyzer=text_to_word)\n",
    "article_by_content = countvec.fit_transform(article_content['doc_full_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take the dot product of the matrix article_by_content with itself, we will get an (n_article x n_article) matrix where n_article is the number of unique articles present in the dataset. Each cell in this matrix will represent how similar an article is to others. Note that in each row, the diagonal elements will be maximum as an article is always most similar with itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = np.dot(article_by_content, article_by_content.transpose()).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3\n",
    "\n",
    "We will now use this similarity matrix to find articles which are similar in content to a given article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_articles(article_id, article_content=article_content, similarity_matrix=similarity_matrix):\n",
    "    \n",
    "    \"\"\"\n",
    "    Given an article id, this function returns a list of articles which are similar to that of the input article\n",
    "    in terms of their content.\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    article_id : str\n",
    "        id of the input article\n",
    "        \n",
    "    article_content : pandas dataframe\n",
    "        a dataframe containing ids and full names of articles\n",
    "        \n",
    "    similarity_matrix : array\n",
    "        an (n_article x n_article) numpy array where n_article is the number of unique articles present in the \n",
    "        dataset and each entry in this array will represent how similar an article is to others \n",
    "        \n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    similar_id : list\n",
    "         ids of the users similar to the input user\n",
    "         \n",
    "    similarity_score : list    \n",
    "         similarity scores of the neighbor users\n",
    "         \n",
    "    similar_dict : dict\n",
    "         a dictionary whose keys are ids of similar users and values are the corresponding similariy scores sorted\n",
    "         according to their similarity scores\n",
    "    \n",
    "    \"\"\"\n",
    "    # find out which row of the dataframe does the input article id belong to\n",
    "    article_row = np.where(article_content['article_id']==article_id)[0][0]\n",
    "    \n",
    "    # find out the row numbers of similar articles\n",
    "    similar_row = list(np.where(similarity_matrix[article_row] > 2)[0])\n",
    "    \n",
    "    # store the corresponding similarity scores\n",
    "    similarity_score = list(similarity_matrix[article_row, similar_row])\n",
    "    \n",
    "     # store the ids of similar articles\n",
    "    similar_id = list(article_content.iloc[similar_row]['article_id'])\n",
    "    \n",
    "    similar_dict = {}\n",
    "    for similar_id,score in zip(similar_id, similarity_score):\n",
    "        similar_dict[similar_id] = score\n",
    "        \n",
    "    # sort the dictionary according to the similarity scores        \n",
    "    similar_dict = {k:v for k,v in sorted(similar_dict.items(), key=lambda x:x[1], reverse=True)}    \n",
    "    \n",
    "    return similar_id, similarity_score, similar_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4\n",
    "\n",
    "Finally we build the function 'make_content_recs' which will make recommendations for a given user based on article content. The entire method for making recommendations can be broken down into the following steps :\n",
    "\n",
    "- find out the articles that our input user has read\n",
    "- for each article that our user has read \n",
    "     - find out other articles that are similar to it \n",
    "     - select only those similar articles which our user has not read yet\n",
    "     - sort these articles according to the number of interactions in descending order\n",
    "     - add them to our recommendation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_content_recs(user_id, article_content=article_content, n=10):\n",
    "    \n",
    "    \"\"\"\n",
    "    A function to make recommendations based on content of the articles. Loops through all the articles that the \n",
    "    input user has interacted with, find articles similar in content for each of them, choose those with which our\n",
    "    user has not interacted yet and recommend them. One important point to note here is that not all articles of \n",
    "    the dataframe 'df' have information about content in the dataframe 'article_content'. Also not all articles\n",
    "    that have a description, have interactions with users (i.e. present in the dataset 'df').\n",
    "    \n",
    "    Parameters\n",
    "    ------------\n",
    "    user_id : int\n",
    "        id of the input user for whom recommendations are to be made\n",
    "         \n",
    "    article_content : pandas dataframe\n",
    "        a dataframe containing ids and full names of articles\n",
    "        \n",
    "    n : int\n",
    "        the number of recommendations we want to make for the input user\n",
    "        \n",
    "    Returns\n",
    "    ---------\n",
    "    rec_ids : list\n",
    "        a list of ids of the recommended articles\n",
    "        \n",
    "    rec_names : list\n",
    "        a list of names of the recommended articles\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # find out the articles that our user has interacted with and also have information about content\n",
    "    articles_read = np.intersect1d(get_user_articles(user_id)[0], article_content['article_id'].unique())\n",
    "    \n",
    "    # array to store our recommendations\n",
    "    recs = np.array([])\n",
    "    \n",
    "    for article in articles_read:\n",
    "        # find out the articles similar in content\n",
    "        similar_articles = list(find_similar_articles(article)[2].keys())[1:]\n",
    "        \n",
    "        # choose articles that have similar content and also no interaction with the user\n",
    "        articles_not_read = np.setdiff1d(similar_articles, articles_read, assume_unique=True)\n",
    "        \n",
    "        # store them in the array\n",
    "        recs = np.unique(np.concatenate([recs, articles_not_read]))\n",
    "       \n",
    "        # if the length of the array exceeds the max. number of recommendations we want to make, break the loop  \n",
    "        if len(recs) > 10:\n",
    "            break\n",
    "            \n",
    "    #  return the first 'n' elements of our recommendation array in case it has more than n entries   \n",
    "    rec_ids = list(recs[:n])   \n",
    "    \n",
    "    # return the names of the recommended articles\n",
    "    rec_names = list(article_content[article_content['article_id'].isin(rec_ids)]['doc_full_name'].values)\n",
    "            \n",
    "    return rec_ids, rec_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make recommendation for some user based on content of articles that the user has already interacted with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_ids, rec_names = make_content_recs(90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following articles are recommended for user 132 on the basis of article content :\n",
      "\n",
      "['This Week in Data Science (September 27, 2016)', 'This Week in Data Science (May 2, 2017)', 'This Week in Data Science (July 26, 2016)', 'This Week in Data Science (December 20, 2016)', 'This Week in Data Science (November 08, 2016)', 'This Week in Data Science (November 15, 2016)', 'This Week in Data Science (February 28, 2017)', 'This Week in Data Science (November 01, 2016)', 'This Week in Data Science (February 7, 2017)', 'This Week in Data Science (July 12, 2016)']\n"
     ]
    }
   ],
   "source": [
    "print(\"The following articles are recommended for user {} on the basis of article content :\".format(132))\n",
    "print()\n",
    "print(rec_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comments**\n",
    "--------------\n",
    "\n",
    "Using this technique, however, we might not be able to provide all users the same number of recommendations and also might even fail to provide recommendations to certain users. This is because of the fact that not all the articles that have interactions with users (as we can see from the dataframe 'df'), have content related information available in the other dataframe 'df_content'. Content based recommendation that we have built above, will only work for articles with content information available. In this situation, we can try to combine user-based collaborative filtering and content based recommendation to provide recommendations to all existing users.\n",
    "\n",
    "One can also try with other features, *e.g.* 'doc_description' to see whether that improves the present  recommendation engine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
